first convolutional layer:  torch.Size([1, 48, 360, 480])
0 denseBlocksDown layer (appended):  torch.Size([1, 128, 360, 480])
0 transDownBlocks layer:  torch.Size([1, 128, 180, 240])
1 denseBlocksDown layer (appended):  torch.Size([1, 208, 180, 240])
1 transDownBlocks layer:  torch.Size([1, 208, 90, 120])
2 denseBlocksDown layer (appended):  torch.Size([1, 288, 90, 120])
2 transDownBlocks layer:  torch.Size([1, 288, 45, 60])
3 denseBlocksDown layer (appended):  torch.Size([1, 368, 45, 60])
3 transDownBlocks layer:  torch.Size([1, 368, 22, 30])
4 denseBlocksDown layer (appended):  torch.Size([1, 448, 22, 30])
4 transDownBlocks layer:  torch.Size([1, 448, 11, 15])
bottleneck layer:  torch.Size([1, 80, 11, 15])
0 to be skipped:  torch.Size([1, 448, 22, 30])
0 transUpBlocks layer:  torch.Size([1, 528, 22, 30])
0 denseBlocksUp layer:  torch.Size([1, 80, 22, 30])
1 to be skipped:  torch.Size([1, 368, 45, 60])
1 transUpBlocks layer:  torch.Size([1, 448, 45, 60])
1 denseBlocksUp layer:  torch.Size([1, 80, 45, 60])
2 to be skipped:  torch.Size([1, 288, 90, 120])
2 transUpBlocks layer:  torch.Size([1, 368, 90, 120])
2 denseBlocksUp layer:  torch.Size([1, 80, 90, 120])
3 to be skipped:  torch.Size([1, 208, 180, 240])
3 transUpBlocks layer:  torch.Size([1, 288, 180, 240])
3 denseBlocksUp layer:  torch.Size([1, 80, 180, 240])
4 to be skipped:  torch.Size([1, 128, 360, 480])
4 transUpBlocks layer:  torch.Size([1, 208, 360, 480])
4 denseBlocksUp layer:  torch.Size([1, 288, 360, 480])
final convolutional layer:  torch.Size([1, 12, 360, 480])
